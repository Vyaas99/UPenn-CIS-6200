{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHqTypMAgQ1F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb284929-9e6a-4340-beaf-d3792947e0e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.1.9)\n",
            "Requirement already satisfied: pinecone-client in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.12.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.0.7-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.27)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.21 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.24)\n",
            "Requirement already satisfied: langchain-core<0.2,>=0.1.26 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.26)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.8)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2024.2.2)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.9.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.0.7)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.4)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.26->langchain) (23.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain) (3.9.15)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Installing collected packages: langchain_openai\n",
            "Successfully installed langchain_openai-0.0.7\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain pinecone-client openai tiktoken langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# import getpass\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"       #@param {type:\"string\"}\n",
        "os.environ[\"PINECONE_API_KEY\"] = \"\"      #@param {type:\"string\"}\n",
        "os.environ[\"PINECONE_ENV\"] = \"gcp-starter\"  #@param {type:\"string\"}\n",
        "\n",
        "from langchain.vectorstores import Pinecone\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.schema import SystemMessage\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langchain.pydantic_v1 import BaseModel, Field, validator\n",
        "import langchain, pinecone\n",
        "\n",
        "from pinecone import Pinecone as pc\n",
        "from pinecone import PodSpec"
      ],
      "metadata": {
        "id": "y9x2G19MhA0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"langchain version: {langchain.__version__}\")\n",
        "print(f\"pinecone version: {pinecone.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlWl9k6bhHdO",
        "outputId": "ac91e35c-9a53-4027-dca6-3e0ce1e28db4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "langchain version: 0.1.9\n",
            "pinecone version: 3.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#+============================= text processing =============================+\n",
        "loader = TextLoader(\"./example.txt\")\n",
        "documents = loader.load()\n",
        "#splitting the text into smaller chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "\n",
        "#+============================== embedding text =============================+\n",
        "llm= OpenAI()\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "pc_ = pc(api_key=os.environ.get(\"PINECONE_API_KEY\"))\n",
        "\n",
        "index_name = \"example-index\"\n",
        "\n",
        "if index_name not in pc_.list_indexes().names():\n",
        "    pc_.create_index(name=index_name, metric=\"cosine\", dimension=1536, spec=PodSpec(environment=\"gcp-starter\") )\n",
        "    docsearch = Pinecone.from_documents(texts, embeddings, index_name=index_name)\n",
        "else:\n",
        "    docsearch = Pinecone.from_existing_index(index_name, embeddings)\n",
        "\n",
        "\n",
        "#+============================== define model ===============================+\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
        "\n",
        "class question_answer(BaseModel):\n",
        "  question: str = Field(..., description = \"Question framed.\")\n",
        "  answer: str = Field(..., description = \"Answer to the question.\")\n",
        "\n",
        "class output(BaseModel):\n",
        "  output: list[question_answer] = []\n",
        "\n",
        "parser = PydanticOutputParser(pydantic_object=output)\n",
        "\n",
        "#+================================== prompt ==================================+\n",
        "prompt = '''You are a dataset creation machine. You make dataset from a given data. Create as much question answer set as you can. Make sure you do not repeat questions and you cover every relevant topic to make the dataset.\n",
        "\n",
        "Data Provided : {text}\n",
        "\n",
        "{format_instructions}\n",
        "\n",
        "Output:'''\n",
        "\n",
        "#+=============================== calling LLM ===============================+\n",
        "dataset = []\n",
        "chat_llm= ChatOpenAI()\n",
        "for text in texts:\n",
        "  _prompt = PromptTemplate(template =  prompt, input_variables = [\"text\"], partial_variables={\"format_instructions\": parser.get_format_instructions()})\n",
        "\n",
        "  _input = _prompt.format_prompt(text = text)\n",
        "  message = [\n",
        "    SystemMessage(content = _input.to_string())\n",
        "  ]\n",
        "\n",
        "  result = chat_llm(message).content\n",
        "\n",
        "  parsed_output = parser.parse(result)\n",
        "  dataset.extend(parsed_output.output)\n",
        "  print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEZRCr1tr8OV",
        "outputId": "22b926ff-194f-4c9a-ec8f-014c1edda40e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
            "  warn_deprecated(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
            "  warn_deprecated(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
            "  warn_deprecated(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[question_answer(question=\"What is Toolformer in the context of Meta AI's recent approach?\", answer=\"Toolformer is Meta AI's recent approach to fusing large language models (LLM) with external APIs.\"), question_answer(question='What does Toolformer combine in terms of programming paradigm?', answer='Toolformer combines zero-shot machine-learning methodology with traditional software interfaces, potentially starting a new programming paradigm.'), question_answer(question='What has led to a new wave of interest in large language models according to the content?', answer='The widespread success of ChatGPT has led to a new wave of interest in large language models.'), question_answer(question=\"How are people reacting to ChatGPT's capabilities on Twitter?\", answer='People are impressed by the capabilities of ChatGPT on Twitter and herald it as the start of a new age of AI.'), question_answer(question='What caution does the author mention about looking at new technology?', answer='The author mentions that it is important to look at new technology without rose-colored glasses and to be aware of the significant issues even with impressive large language models (LLMs).')]\n",
            "[question_answer(question=\"What is Toolformer in the context of Meta AI's recent approach?\", answer=\"Toolformer is Meta AI's recent approach to fusing large language models (LLM) with external APIs.\"), question_answer(question='What does Toolformer combine in terms of programming paradigm?', answer='Toolformer combines zero-shot machine-learning methodology with traditional software interfaces, potentially starting a new programming paradigm.'), question_answer(question='What has led to a new wave of interest in large language models according to the content?', answer='The widespread success of ChatGPT has led to a new wave of interest in large language models.'), question_answer(question=\"How are people reacting to ChatGPT's capabilities on Twitter?\", answer='People are impressed by the capabilities of ChatGPT on Twitter and herald it as the start of a new age of AI.'), question_answer(question='What caution does the author mention about looking at new technology?', answer='The author mentions that it is important to look at new technology without rose-colored glasses and to be aware of the significant issues even with impressive large language models (LLMs).'), question_answer(question=\"What is OpenAI's approach to training language models?\", answer='OpenAI aims to train language models to produce natural-sounding text.'), question_answer(question='What is RLHF (reinforcement learning from human feedback) and how is it used by OpenAI?', answer='RLHF is introduced by OpenAI through InstructGPT and ChatGPT to ensure that the models generate text aligning with human expectations.'), question_answer(question='Does RLHF guarantee that language models verify the accuracy of their outputs?', answer='No, RLHF does not guarantee that the models verify the accuracy of their outputs.'), question_answer(question='What recent release did Microsoft make in the AI space?', answer='Microsoft released the public beta testing version of their new Bing AI search engine.'), question_answer(question=\"How can users interact with Microsoft's new Bing AI search engine?\", answer='Users who have signed up and received invitations can now interact with the chat-based search engine.'), question_answer(question=\"What data source does Microsoft's Bing AI search engine use to retrieve information?\", answer=\"Microsoft's Bing AI search engine uses web search to retrieve information and generate text based on it.\"), question_answer(question=\"What issue arises with Microsoft's Bing AI search engine when there isn't enough data available?\", answer=\"It produces false information when there isn't enough data available.\"), question_answer(question=\"Why is relying solely on web search considered insufficient for Microsoft's Bing AI search engine?\", answer='Relying solely on web search is not sufficient because it can lead to false information, making it seem like a patch on the problem.')]\n",
            "[question_answer(question=\"What is Toolformer in the context of Meta AI's recent approach?\", answer=\"Toolformer is Meta AI's recent approach to fusing large language models (LLM) with external APIs.\"), question_answer(question='What does Toolformer combine in terms of programming paradigm?', answer='Toolformer combines zero-shot machine-learning methodology with traditional software interfaces, potentially starting a new programming paradigm.'), question_answer(question='What has led to a new wave of interest in large language models according to the content?', answer='The widespread success of ChatGPT has led to a new wave of interest in large language models.'), question_answer(question=\"How are people reacting to ChatGPT's capabilities on Twitter?\", answer='People are impressed by the capabilities of ChatGPT on Twitter and herald it as the start of a new age of AI.'), question_answer(question='What caution does the author mention about looking at new technology?', answer='The author mentions that it is important to look at new technology without rose-colored glasses and to be aware of the significant issues even with impressive large language models (LLMs).'), question_answer(question=\"What is OpenAI's approach to training language models?\", answer='OpenAI aims to train language models to produce natural-sounding text.'), question_answer(question='What is RLHF (reinforcement learning from human feedback) and how is it used by OpenAI?', answer='RLHF is introduced by OpenAI through InstructGPT and ChatGPT to ensure that the models generate text aligning with human expectations.'), question_answer(question='Does RLHF guarantee that language models verify the accuracy of their outputs?', answer='No, RLHF does not guarantee that the models verify the accuracy of their outputs.'), question_answer(question='What recent release did Microsoft make in the AI space?', answer='Microsoft released the public beta testing version of their new Bing AI search engine.'), question_answer(question=\"How can users interact with Microsoft's new Bing AI search engine?\", answer='Users who have signed up and received invitations can now interact with the chat-based search engine.'), question_answer(question=\"What data source does Microsoft's Bing AI search engine use to retrieve information?\", answer=\"Microsoft's Bing AI search engine uses web search to retrieve information and generate text based on it.\"), question_answer(question=\"What issue arises with Microsoft's Bing AI search engine when there isn't enough data available?\", answer=\"It produces false information when there isn't enough data available.\"), question_answer(question=\"Why is relying solely on web search considered insufficient for Microsoft's Bing AI search engine?\", answer='Relying solely on web search is not sufficient because it can lead to false information, making it seem like a patch on the problem.'), question_answer(question='What solution does Meta AI present for LLMs to use external tools?', answer='Meta AI presents a solution that allows LLMs to use external tools via simple APIs, achieving the best of both worlds.'), question_answer(question='What tools does Toolformer integrate according to the recent paper by Meta AI?', answer='Toolformer integrates a range of tools, including a calculator, a Q&A system, two search engines, a translation system, and a calendar.'), question_answer(question='What is the focus of the deep dive into the cutting-edge framework discussed in the paper?', answer='The deep dive focuses on how the cutting-edge framework can solve crucial LLM problems.'), question_answer(question='What issue is mentioned when using ChatGPT in the provided content?', answer='When using ChatGPT, the issue mentioned is that it often hallucinates, providing non-existing methods, facts, and citations.'), question_answer(question='Can you provide an example of ChatGPT hallucinating while discussing recent trends in federated learning?', answer='An example provided is when ChatGPT was asked to cite a study related to federated learning in healthcare, but it provided non-existing information.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A sample of the question and answer dataset is shown below"
      ],
      "metadata": {
        "id": "zf1ulMmPCI6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  print(dataset[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZwX_rvSB0f0",
        "outputId": "bb5a24cf-ba5d-4bf2-9bc1-2cf8cb6bda14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "question=\"What is Toolformer in the context of Meta AI's recent approach?\" answer=\"Toolformer is Meta AI's recent approach to fusing large language models (LLM) with external APIs.\"\n",
            "question='What does Toolformer combine in terms of programming paradigm?' answer='Toolformer combines zero-shot machine-learning methodology with traditional software interfaces, potentially starting a new programming paradigm.'\n",
            "question='What has led to a new wave of interest in large language models according to the content?' answer='The widespread success of ChatGPT has led to a new wave of interest in large language models.'\n",
            "question=\"How are people reacting to ChatGPT's capabilities on Twitter?\" answer='People are impressed by the capabilities of ChatGPT on Twitter and herald it as the start of a new age of AI.'\n",
            "question='What caution does the author mention about looking at new technology?' answer='The author mentions that it is important to look at new technology without rose-colored glasses and to be aware of the significant issues even with impressive large language models (LLMs).'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#+================================= QA Bot ==================================+\n",
        "prompts=[\"who is meta?\",\"what is a toolformer?\",\"what is an llm?\"]\n",
        "\n",
        "for i in range(3):\n",
        "  if not prompts:\n",
        "    question = input(f\"Question {i+1}> \")\n",
        "  else:\n",
        "    question=prompts[i]\n",
        "    print(f\"Question {i+1}:{question}\")\n",
        "  docs = docsearch.similarity_search(question)\n",
        "  respond = chain.run(input_documents=docs, question=question)\n",
        "  print(f\"Response {i+1}:{respond}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7oxeTkzjFoe",
        "outputId": "d35d50b4-2a13-4d6a-8221-085ea70915c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question 1:who is meta?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response 1:\n",
            "Meta is the company that produced the recent paper discussing the solution for LLMs using external tools via simple APIs. They also developed Toolformer, a framework that integrates various tools to solve LLM problems.\n",
            "Question 2:what is a toolformer?\n",
            "Response 2:\n",
            "\n",
            "A toolformer is a framework developed by Meta AI that integrates large language models with external tools through simple APIs. It aims to solve issues with LLMs that often produce false or inaccurate information by incorporating a variety of tools, such as a calculator, Q&A system, search engines, translation system, and calendar.\n",
            "Question 3:what is an llm?\n",
            "Response 3:\n",
            "An LLM is a large language model, a type of artificial intelligence technology that uses machine learning to generate natural-sounding text.\n"
          ]
        }
      ]
    }
  ]
}