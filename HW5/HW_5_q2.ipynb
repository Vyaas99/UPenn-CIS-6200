{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75TGzuQ5w71v",
        "outputId": "6a3119fb-2677-407f-f43e-47b0e6984bfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.1.9-py3-none-any.whl (816 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m817.0/817.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pinecone-client\n",
            "  Downloading pinecone_client-3.1.0-py3-none-any.whl (210 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.0/211.0 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai\n",
            "  Downloading openai-1.12.0-py3-none-any.whl (226 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain_openai\n",
            "  Downloading langchain_openai-0.0.7-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.27)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.21 (from langchain)\n",
            "  Downloading langchain_community-0.0.24-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2,>=0.1.26 (from langchain)\n",
            "  Downloading langchain_core-0.1.26-py3-none-any.whl (246 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.4/246.4 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.2.0,>=0.1.0 (from langchain)\n",
            "  Downloading langsmith-0.1.8-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.2/62.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2024.2.2)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.9.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.0.7)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.26->langchain) (23.2)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.0->langchain)\n",
            "  Downloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: pinecone-client, orjson, mypy-extensions, marshmallow, jsonpointer, h11, typing-inspect, tiktoken, jsonpatch, httpcore, langsmith, httpx, dataclasses-json, openai, langchain-core, langchain_openai, langchain-community, langchain\n",
            "Successfully installed dataclasses-json-0.6.4 h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.9 langchain-community-0.0.24 langchain-core-0.1.26 langchain_openai-0.0.7 langsmith-0.1.8 marshmallow-3.20.2 mypy-extensions-1.0.0 openai-1.12.0 orjson-3.9.15 pinecone-client-3.1.0 tiktoken-0.6.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain pinecone-client openai tiktoken langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# import getpass\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"       #@param {type:\"string\"}\n",
        "os.environ[\"PINECONE_API_KEY\"] = \"\"      #@param {type:\"string\"}"
      ],
      "metadata": {
        "id": "qmaq8LesxB65"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationSummaryMemory, ChatMessageHistory\n",
        "from langchain_openai import OpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "from langchain.vectorstores import Pinecone\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.schema import SystemMessage\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langchain.pydantic_v1 import BaseModel, Field, validator\n",
        "import langchain, pinecone\n",
        "\n",
        "from pinecone import Pinecone as pc\n",
        "from pinecone import PodSpec\n"
      ],
      "metadata": {
        "id": "I0q1hlugxI45"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store = {}"
      ],
      "metadata": {
        "id": "js6RJB0ocllA"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#+================================== prompt ==================================+\n",
        "prompt = '''You will first make a greeting to me and answer my question. All reponses should be based on my information: \"{info}\".\n",
        "\n",
        "If there is no name in the information, you should say: Hi, {id}!\n",
        "\n",
        "If there is a name in the information, you should greet me with my name.\n",
        "\n",
        "Here is my question: {question}\n",
        "\n",
        "Output:'''\n",
        "\n",
        "#+=============================== calling LLM ===============================+\n",
        "llm = OpenAI(temperature=0)\n",
        "\n",
        "_prompt = PromptTemplate(template =  prompt, input_variables = [\"info\", \"question\"])\n",
        "\n",
        "while True:\n",
        "  print(\"=\"*10, f\"New session\", \"=\"*10)\n",
        "\n",
        "  id = input(f\"Please enter your id: \")          # id for retreving information\n",
        "  if id.lower() in ['q','quit','exit']:\n",
        "    break\n",
        "\n",
        "  if id not in store:                    # record info\n",
        "    info = input(f\"Please tell me something about you, this helps me improve the answer: \")\n",
        "    store[id] = info\n",
        "  else:\n",
        "    info = store[id]                    # using previous info\n",
        "\n",
        "  while True:\n",
        "    memory = ConversationSummaryMemory(llm=OpenAI())\n",
        "    conversation_with_summary = ConversationChain(\n",
        "      llm=llm,\n",
        "      memory=memory,\n",
        "      verbose=False\n",
        "    )\n",
        "    user_input = input(f\"{id} > \")\n",
        "\n",
        "    if user_input.lower() in [\"q\", \"quit\", \"exit\"]:    # quit the current conversation\n",
        "      new_info = memory.predict_new_summary(memory.chat_memory.messages, info)\n",
        "      store[id] = new_info\n",
        "      break\n",
        "\n",
        "    _input = _prompt.format_prompt(id = id, info = info, question = user_input).to_string()\n",
        "\n",
        "    result = conversation_with_summary.predict(input = _input)\n",
        "    print(result)\n",
        "\n",
        "  if user_input.lower() == \"exit\":               # quit the QA system\n",
        "    print(\"=\"*10, f\"Thank you for using!\", \"=\"*10)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4TcadJdxGMH",
        "outputId": "762a3533-959d-465c-aae2-e405224970a0"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========== New session ==========\n",
            "Please enter your id: vyaas\n",
            "Please tell me something about you, this helps me improve the answer: I am a non vegetarian.\n",
            "vyaas > What should I eat for lunch?\n",
            " Hi, vyaas! As a non vegetarian, you have many options for lunch. You could have a chicken sandwich, a beef stir fry, or a tuna salad. Do any of those sound appealing to you?\n",
            "vyaas > q\n",
            "========== New session ==========\n",
            "Please enter your id: messi\n",
            "Please tell me something about you, this helps me improve the answer: I am a vegetarian.\n",
            "messi > What should I eat for lunch?\n",
            " Hi, messi! As a vegetarian, you have many options for lunch. You could have a salad with fresh greens, vegetables, and a protein source like tofu or beans. Another option could be a veggie wrap with hummus, avocado, and your choice of veggies. You could also try a quinoa and vegetable stir-fry or a lentil soup. The possibilities are endless! Is there a specific type of cuisine you are in the mood for?\n",
            "messi > q\n",
            "========== New session ==========\n",
            "Please enter your id: vyaas\n",
            "vyaas > What should I eat for lunch?\n",
            " Hi, vyaas! As a non-vegetarian, I do not have any dietary restrictions. For lunch, you could try a chicken sandwich or a tuna salad. Or if you're feeling adventurous, you could try a new dish like sushi or a lamb curry. It all depends on your personal preferences and cravings. Do you have any specific cuisine in mind?\n",
            "vyaas > exit\n",
            "========== Thank you for using! ==========\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "store"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKkrqgBCxegw",
        "outputId": "be8a05e3-0c02-4db0-9085-856d8e930152"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'vyaas': '\\nThe human asks if the AI has any dietary restrictions. The AI responds that it is a non-vegetarian. The human then asks why the AI is not a vegetarian. The AI explains that it does not require sustenance in the traditional sense and does not have the same ethical considerations as humans.',\n",
              " 'messi': '\\nThe human asks the AI if it has any dietary restrictions. The AI responds that it is a vegetarian.'}"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## backup code"
      ],
      "metadata": {
        "id": "gDQkjWL-xjS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#+============================= text processing =============================+\n",
        "loader = TextLoader(\"./example.txt\")\n",
        "documents = loader.load()\n",
        "#splitting the text into smaller chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "\n",
        "#+============================== embedding text =============================+\n",
        "llm= OpenAI()\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "pc_ = pc(api_key=os.environ.get(\"PINECONE_API_KEY\"))\n",
        "\n",
        "index_name = \"example-index\"\n",
        "\n",
        "if index_name not in pc_.list_indexes().names():\n",
        "    pc_.create_index(name=index_name, metric=\"cosine\", dimension=1536, spec=PodSpec(environment=\"gcp-starter\") )\n",
        "    docsearch = Pinecone.from_documents(texts, embeddings, index_name=index_name)\n",
        "else:\n",
        "    docsearch = Pinecone.from_existing_index(index_name, embeddings)\n",
        "\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
        "\n",
        "#+================================= QA Bot ==================================+\n",
        "prompts=[\"who is meta?\",\"what is a toolformer?\",\"what is an llm?\"]\n",
        "\n",
        "for i in range(3):\n",
        "  if not prompts:\n",
        "    question = input(f\"Question {i+1}> \")\n",
        "  else:\n",
        "    question=prompts[i]\n",
        "    print(f\"Question {i+1}:{question}\")\n",
        "  docs = docsearch.similarity_search(question)\n",
        "  respond = chain.run(input_documents=docs, question=question)\n",
        "  print(f\"Response {i+1}:{respond}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLlOXmonxfzf",
        "outputId": "ddf0984f-4ed1-482d-e1ce-25781d3bb5dd"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question 1:who is meta?\n",
            "Response 1:\n",
            "\n",
            "Meta is a company or organization that published the paper discussing a solution for LLMs using external tools through APIs.\n",
            "Question 2:what is a toolformer?\n",
            "Response 2:\n",
            "\n",
            "A toolformer is a framework developed by Meta AI that allows large language models to use external tools through simple APIs, combining the benefits of both LLMs and traditional software interfaces. It integrates a range of tools, including a calculator, Q&A system, search engines, translation system, and calendar.\n",
            "Question 3:what is an llm?\n",
            "Response 3:\n",
            "LLM stands for large language models. These are artificial intelligence systems that are trained on large amounts of text data and can generate natural-sounding text. However, they may still have issues with accuracy and generating false information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Supplemental Q2"
      ],
      "metadata": {
        "id": "IDMj3bjrh6GU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We can either use counter to allow maximum questions to break a loop or ask user to enter close\n",
        "flag=True\n",
        "print(\"Enter close to exit\\n\\n\")\n",
        "while True:\n",
        "  question=input(\"User:\\n\")\n",
        "  if question.lower()==\"close\":\n",
        "    break\n",
        "  if flag:\n",
        "    first_q=question\n",
        "    print(\"Response: Before I answer your question, may I know your name?\\n\")\n",
        "    question=input(\"User:\")\n",
        "    name=llm(f\"Extract the name from the sentence:{question}\")[2:]\n",
        "    response=chain.run(input_documents=docs, question=first_q)\n",
        "    print(f\"Response: Hello, {name}. Hope you are having a good day so far. {response}\\n\")\n",
        "    flag=False\n",
        "  else:\n",
        "    response=chain.run(input_documents=docs, question=question)\n",
        "    print(f\"Response: Hello, {name}. Hope you are having a good day so far. {response}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzJ19zNJh4uD",
        "outputId": "040d643e-181d-42b2-ece8-88cffc306954"
      },
      "execution_count": 43,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter close to exit\n",
            "\n",
            "\n",
            "User:\n",
            "what is a toolformer?\n",
            "Response: Before I answer your question, may I know your name?\n",
            "\n",
            "User:Vyaas\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response: Hello, Vyaas. Hope you are having a good day so far. \n",
            "A toolformer is a framework developed by Meta AI that allows large language models (LLMs) to use external tools through simple APIs, combining the strengths of both approaches. It integrates various tools such as a calculator, Q&A system, search engines, translation system, and calendar to help address the limitations of LLMs in generating factual and accurate information.\n",
            "\n",
            "User:\n",
            "who is meta?\n",
            "Response: Hello, Vyaas. Hope you are having a good day so far. \n",
            "\n",
            "Meta AI is the company that presented the solution for LLMs to use external tools via simple APIs, called Toolformer. They also integrate a range of tools, including a calculator, a Q&A system, two search engines, a translation system, and a calendar.\n",
            "\n",
            "User:\n",
            "close\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optional: Multiple users attempt"
      ],
      "metadata": {
        "id": "dUZtrwatxo14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm=OpenAI()\n",
        "print(\"Enter \\\"close\\\" to exit as a user\\nEnter \\\"quit\\\" to stop running the cell\\n\\n\")\n",
        "names=[]\n",
        "while True:\n",
        "  flag=True\n",
        "  while True:\n",
        "    question=input(\"User:\")\n",
        "\n",
        "    #Checks whether the user wants to exit or quit\n",
        "    if question.lower()==\"close\" or question.lower()==\"quit\":\n",
        "      #Breaks the inner loop but cell may keep running\n",
        "      print(\"\\nUser Exiting\")\n",
        "      print(\"=\"*20,\"\\n\")\n",
        "      break\n",
        "\n",
        "    #Checks if this is the first question by the user in the current session\n",
        "    if flag:\n",
        "      first_q=question\n",
        "      print(\"Response: Before I answer your question, may I know your name?\\n\")\n",
        "      question=input(\"User:\")\n",
        "      name=llm(f\"Extract the name from the sentence:{question}\")[2:]\n",
        "      response=chain.run(input_documents=docs, question=first_q)\n",
        "      #Prints different outputs if this is the first time the QA bot is meeting this person\n",
        "      if name in names:\n",
        "        print(f\"Response: Hello, {name}. It is nice to see you again. Hope you are having a good day so far. {response}\\n\")\n",
        "      else:\n",
        "        names.append(name)\n",
        "        print(f\"Response: Hello, {name}. It is nice to meet you. Hope you are having a good day so far. {response}\\n\")\n",
        "\n",
        "      flag=False\n",
        "\n",
        "    #Response if it is not the first question by the user in the current session\n",
        "    else:\n",
        "      response=chain.run(input_documents=docs, question=question)\n",
        "      print(f\"Response: Hello, {name}. Hope you are having a good day so far. {response}\\n\")\n",
        "  if question.lower()==\"quit\":\n",
        "    #Breaks the outer loop and the cell stops running\n",
        "    print(\"\"*20)\n",
        "    print(\"Over and out!\")\n",
        "    print(\"\"*20)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KD9aJz8sxnKu",
        "outputId": "070f7e75-4a24-42c3-cfc0-a19a1fe89e31"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter \"close\" to exit as a user\n",
            "Enter \"quit\" to stop running the cell\n",
            "\n",
            "\n",
            "User:what is a toolformer?\n",
            "Response: Before I answer your question, may I know your name?\n",
            "\n",
            "User:My name is Vyaas\n",
            "Response: Hello, Vyaas. It is nice to meet you. Hope you are having a good day so far. \n",
            "A toolformer is a framework developed by Meta AI that integrates a range of external tools, such as a calculator, Q&A system, search engines, translation system, and a calendar, to enhance the capabilities of large language models (LLMs). This approach aims to combine the benefits of LLMs with traditional software interfaces to solve crucial problems in natural language processing.\n",
            "\n",
            "User:close\n",
            "\n",
            "User Exiting\n",
            "==================== \n",
            "\n",
            "User:what is a toolformer?\n",
            "Response: Before I answer your question, may I know your name?\n",
            "\n",
            "User:The name is Ronaldo.\n",
            "Response: Hello, Ronaldo. It is nice to meet you. Hope you are having a good day so far.  A toolformer is a framework developed by Meta AI that allows large language models to use external tools through simple APIs. It aims to solve issues with LLMs, such as producing false information, by integrating various tools such as a calculator, Q&A system, search engines, translation system, and calendar.\n",
            "\n",
            "User:close\n",
            "\n",
            "User Exiting\n",
            "==================== \n",
            "\n",
            "User:who is meta?\n",
            "Response: Before I answer your question, may I know your name?\n",
            "\n",
            "User:My name is Vyaas\n",
            "Response: Hello, Vyaas. It is nice to see you again. Hope you are having a good day so far. \n",
            "Meta is a company that has developed a solution for large language models (LLMs) to use external tools via simple APIs, called Toolformer. They also use their technology in their own ChatGPT tool.\n",
            "\n",
            "User:quit\n",
            "\n",
            "User Exiting\n",
            "==================== \n",
            "\n",
            "\n",
            "Over and out!\n",
            "\n"
          ]
        }
      ]
    }
  ]
}